{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Perceptron, Logistic Regression, Stocashtic Gradient Descent\n",
    "\n",
    "\n",
    "\n",
    "This assignment is due on Moodle by **11:59pm on Friday Oct 4**. \n",
    "Your solutions to theoretical questions should be done in Markdown/MathJax directly below the associated question.\n",
    "Your solutions to computational questions should include any specified Python code and results \n",
    "as well as written commentary on your conclusions.\n",
    "Remember that you are encouraged to discuss the problems with your instructors and classmates, \n",
    "but **you must write all code and solutions on your own**. For a refresher on the course **Collaboration Policy** click [here](https://github.com/BoulderDS/CSCI5622-Machine-Learning/blob/master/info/syllabus.md#collaboration-policy).\n",
    "\n",
    "**NOTES**: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda (Version: 2019.07) with Python 3.7. \n",
    "- Some problems with code may be autograded.  If we provide a function API **do not** change it.  If we do not provide a function API then you're free to structure your code however you like. \n",
    "- Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. \n",
    "- Extra credit questions will not make your homework total scores overflow i.e., not exceed 35% in the final grade. But you can use extra credit in one homework to cover another.\n",
    "\n",
    "**Acknowledgment** : Chris Ketelsen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please put your name and cuidentity username.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Name**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [40 points] Problem 1 - Perceptron Training\n",
    "\n",
    "Consider a binary classification problem on the following dataset:\n",
    "\n",
    "| x1   | x2         | x3      | y| \n",
    "|:------:|:------------:| :-----------:|---:|\n",
    "|0|0|0|-1|\n",
    "|0|0|1|1|\n",
    "|0|1|0|1|\n",
    "|1|0|0|1|\n",
    "|0|1|1|-1|\n",
    "|1|1|0|-1|\n",
    "|1|0|1|-1|\n",
    "|1|1|1|1|\n",
    "\n",
    "We are going to experiment with the Perceptron algorithm in this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1 [5 points]:** Complete the `perceptron_train` function and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change - unless needed\n",
    "data = np.array([\n",
    "    [0, 0, 0, 1, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 0, 1, 1, 0, 1],\n",
    "    [0, 1, 0, 0, 1, 0, 1, 1],\n",
    "    [-1, 1, 1, 1, -1, -1, -1, 1]\n",
    "])\n",
    "data = np.transpose(data)\n",
    "# Initialize the weights and bias (note that we use a non-standard initialization here).\n",
    "weights = np.array([0, 0.5, 0.5])\n",
    "bias = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cf3429842841ca63f96889263056749",
     "grade": false,
     "grade_id": "cell-583db73f125b72af",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def perceptron_train(data, weights, bias):\n",
    "    \"\"\"\n",
    "    apply transformation and update weights and bias\n",
    "    :type X: array\n",
    "    :type y: int\n",
    "    :type weights: array\n",
    "    :type bias : int\n",
    "    :rtype: weights, bias,number of mistakes\n",
    "    \"\"\"\n",
    "    mistakes_count = 0\n",
    "    for row in data:\n",
    "        X = np.array(row[:3])\n",
    "        y = row[-1]\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    return weights, bias, mistakes_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1 A [2.5 points]:** Report the weights, bias and number of mistakes after the first epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do not change or remove this code\n",
    "weights, bias, number_of_mistakes = perceptron_train(data, weights, bias)\n",
    "print(f'weights: {weights}, bias: {bias}, mistakes: {number_of_mistakes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4443206bb8030ffda6798560f3358ec",
     "grade": true,
     "grade_id": "cell-71a254a22738d8dd",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# for grading - ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1 B [2.5 points]:**  Run the perceptron training for 50 more epochs with the updated weights and report the weights, bias and number of mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac6b3485d7a8339a44998a1b549c3dba",
     "grade": false,
     "grade_id": "cell-9162930c9497743a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "for i in range(epochs):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "print(f'weights: {weights}, bias: {bias}, mistakes: {number_of_mistakes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9fc7e537a12e005eaf453323c97b8d5b",
     "grade": true,
     "grade_id": "cell-959e42c284018b8d",
     "locked": true,
     "points": 2.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# for grading - ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d728b9f90e01d970fd2163eada109dc2",
     "grade": false,
     "grade_id": "cell-fdb42b2a6b9cefea",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Part 2 [5 points]:** Is it possible that your Perceptron classifier would \n",
    "    ever perfectly classify all training examples after more passes of the Perceptron Algorithm?\n",
    "    Clearly explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aab341a9986cebe8f0e3a28dd5841d0e",
     "grade": true,
     "grade_id": "cell-9e4148ad96554da5",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7dd7f9627e5f43c3761fe6baba15ee9",
     "grade": false,
     "grade_id": "cell-91fc1b93bd24f56d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Part 3 [5 points]:** Does the Perceptron classifier necessarily make the same number of mistakes after the first epoch if the data is presented in any other randomized order? \n",
    "    Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "008162ff8ebf6900421370575c9e7014",
     "grade": true,
     "grade_id": "cell-47e30b7cd8370b85",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Part 4 [15 Points]:  Perceptron Classifier on random generated data\n",
    "\n",
    "Update the Perceptron Learning Algorithm to explore the convergence on linearly separable simulated data sets with particular properties. Take a look at the `Perceptron` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ca135bbd064fc82b2a3d5b5c950c25f",
     "grade": false,
     "grade_id": "cell-7770ef2854032d8b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    Class to fit a perceptron classifier to simulated data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n=100, margin=0.1, X=None, y=None, random_state=1241):\n",
    "        \"\"\"\n",
    "        Initializes Perceptron class.  Generates training data and sets parameters. \n",
    "\n",
    "        :param n: the number of training examples\n",
    "        :param margin: the margin between decision boundary and data\n",
    "        :param random_state: seed for random number generator \n",
    "        :param X: Input training features.  Only used for unit testing. \n",
    "        :param y: Input training labels.  Only used for unit testing. \n",
    "        \"\"\"\n",
    "        # initalize random seed\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "        # initialize parameters\n",
    "        self.n, self.M = n, margin\n",
    "\n",
    "        # generate random simulated data\n",
    "        self.X_train, self.y_train = self.gen_data()\n",
    "\n",
    "        # only used for unit tests\n",
    "        if X is not None and y is not None:\n",
    "            self.X_train, self.y_train, self.n = X, y, X.shape[0]\n",
    "\n",
    "        # initialize weights and bias\n",
    "        self.w = np.array([1.0, 0.0])\n",
    "        self.b = 0\n",
    "\n",
    "        # initialize total mistake counter\n",
    "        self.num_mistakes = 0\n",
    "\n",
    "    def train(self, max_epochs=100):\n",
    "        \"\"\"\n",
    "        Runs the Perceptron Algorithm until all training data is correctly classified. \n",
    "\n",
    "        :param max_epochs: Maximum number of epochs to perform before stopping.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def plot_model(self, decision_boundary=False):\n",
    "        \"\"\"\n",
    "        Plots the simulated data.  Plots the learned decision boundary (#TODO) \n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n",
    "        colors = [\"steelblue\" if yi == -\n",
    "                  1 else \"#a76c6e\" for yi in self.y_train]\n",
    "        ax.scatter(self.X_train[:, 0], self.X_train[:, 1], color=colors, s=75)\n",
    "        if decision_boundary:\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "        ax.grid(alpha=0.25)\n",
    "        ax.set_xlabel(r\"$x_1$\", fontsize=16)\n",
    "        ax.set_ylabel(r\"$x_2$\", fontsize=16)\n",
    "\n",
    "    def gen_data(self):\n",
    "        \"\"\"\n",
    "        Generate random linearly separable data with given margin. \n",
    "        Note: You should not need to change this function \n",
    "        \"\"\"\n",
    "        flip = np.random.choice([-1, 1])\n",
    "        pos_x1 = np.random.uniform(-1 / np.sqrt(2), 1 / np.sqrt(2), int(self.n / 2))\n",
    "        pos_x2 = np.random.uniform(\n",
    "            self.M + flip * 0.1, 1 / np.sqrt(2), int(self.n / 2))\n",
    "        pos_x2[-1] = self.M + flip * 0.1\n",
    "        neg_x1 = np.random.uniform(-1 / np.sqrt(2), 1 / np.sqrt(2), int(self.n / 2))\n",
    "        neg_x2 = np.random.uniform(-1 / np.sqrt(2), -\n",
    "                                   self.M + flip * 0.1, int(self.n / 2))\n",
    "        neg_x2[-1] = -self.M + flip * 0.1\n",
    "        X = np.concatenate((np.column_stack((pos_x1, pos_x2)),\n",
    "                            np.column_stack((neg_x1, neg_x2))))\n",
    "        X = np.dot(X, np.array(\n",
    "            [[np.cos(np.pi / 6), np.sin(np.pi / 6)], [-np.sin(np.pi / 6), np.cos(np.pi / 6)]]))\n",
    "        y = np.array([+1] * int(self.n / 2) + [-1] * int(self.n / 2))\n",
    "        rand_order = np.random.choice(\n",
    "            range(self.n), replace=False, size=self.n)\n",
    "        return X[rand_order], y[rand_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4 A [5 points]:** The `Perceptron` class above has the capability of generating its own training data with certain properties. Execute the cell below to generate $n=100$ simulated training examples and plot them. Experiment with the `margin` parameter (good values to try are between $0.01$ and $0.4$). Explain what the `margin` parameter is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 0.2 # subject to experiment\n",
    "perc = Perceptron(n=100, margin=margin)\n",
    "perc.plot_model(decision_boundary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50441211d139b2a4f9da7cd7ab870d31",
     "grade": true,
     "grade_id": "cell-788872a24aff264a",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": false
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4 B [5 points]:** Modify the `train` method in the `Perceptron` class to perform the Perceptron Learning Algorithm and learn weights ${\\bf w}$ and bias $b$ that perfectly classify the linearly separable training data. Your implementation should:\n",
    "\n",
    "- Visit all training examples in a random shuffled order over each training epoch.\n",
    "- Terminate when you finish an epoch without making a single classification error or when you hit the maximum number of epochs.\n",
    "- Use the `self.num_mistakes` counter to count the total number of classification errors over the entire training process.\n",
    "\n",
    "\n",
    "Notes:\n",
    "\n",
    "You should not use Scikit-Learn's Perceptron object in your solution.\n",
    "It's a good idea to implement a stopping criterion based on the `max_epochs` parameter as the first step. Later we'll look at training sets that will terminate on their own, but implementing a stopping mechanism will save you some pain in the development process.\n",
    "Do not change the initial guess for the weights and bias. These values were chosen to match the example done in lecture for the unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6333832632b3cc873c66bb10c9a42266",
     "grade": true,
     "grade_id": "cell-ad1677859c4ce262",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from tests import tests\n",
    "tests.run_test_suite('prob 1.4B', Perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4 C [5 points]:** Modify the `plot_model` method so that it plots the learned decision boundary with the training data. Demonstrate that your method is working by training a perceptron with a margin of your choice and displaying the resulting plot. What is the equation of the learned decision boundary? (Use symbols not numbers.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfd198608dd122cc1be160a9766b8d25",
     "grade": true,
     "grade_id": "cell-807e6237e11d03ed",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = Perceptron(n=100, margin=0.2)\n",
    "perc.train()\n",
    "perc.plot_model(decision_boundary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 5 [10 points]** : \n",
    "\n",
    "Verify the theorem which states that, if you train a perceptron on linearly separable training data with margin $M > 0$ and each training point satisfies $\\|{\\bf x}\\|_2 \\leq 1$ then the Perceptron algorithm will complete after making at most $1/M^2$ classification mistakes.\n",
    "\n",
    "Do the following to verify the above statement: \n",
    "- Train perceptrons with $n = 100$ and different margins ($M=0.3, 0.1, 0.01, 0.001,$ and $0.0001$).\n",
    "- Produce a log-log plot with $1/M$ on the horizontal axis and the total numbers of mistakes on the vertical axis. \n",
    "- On the same set of axes, plot the theoretical upper bound on the number of training mistakes.\n",
    "\n",
    "Usually we run multiple simulations and get an averaged total number of mistakes for each margin, but it is fine if you only do once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a51d12ddace6cca4e1138f86b7ac6114",
     "grade": true,
     "grade_id": "cell-5eb12c673017afa0",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra Credit [5 points]** : \n",
    "\n",
    "* Explain the limitations of the vanilla perceptron implemented above.\n",
    "* Provide pseudo code/steps to implement Voting Perceptron or Average Perceptron (see the textbook) and discuss the pros and cons (Compare space complexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f845d3da91814bdd23a2bf0699bf8fba",
     "grade": true,
     "grade_id": "cell-945d3bde845764bf",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [60 points] Problem 2 : Logistic Regression + SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Problem, you'll implement a Logistic Regression classifier to predict whether person on the Titanic will survive or not.\n",
    "\n",
    "\n",
    "Dataset has following attributes:\n",
    "\n",
    "|Variable|Definition|Key|\n",
    "|:----:|:----:|:---|\n",
    "|survival|\tSurvival|\t0 = No, 1 = Yes|\n",
    "|pclass|\tTicket class|\t1 = 1st, 2 = 2nd, 3 = 3rd|\n",
    "|Age |Age in years\t||\n",
    "|fare\t|Passenger fare|\t|\n",
    "|sibsp\t|# of siblings / spouses aboard the Titanic| |\n",
    "|parch\t|# of parents / children aboard the Titanic\t||\n",
    "|sex\t| Sex|\tone hot encoded male, female|\n",
    "|embarked | Port of Embarkation | one hot encoded C = Cherbourg, Q = Queenstown, S = Southampton|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is a class to load the titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have install pandas and numpy before you run.\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import exp, log\n",
    "from collections import defaultdict\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Class to load dataset containing titanic survival features\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, location, random_state=1241):\n",
    "        # You shouldn't have to modify this class, but you can if you'd like\n",
    "        # Load the dataset\n",
    "        np.random.seed(random_state)\n",
    "        f = gzip.open(location, 'rb')\n",
    "        self.train_x, self.train_y, self.test_x, self.test_y = pickle.load(f)\n",
    "        # appending biases\n",
    "        self.train_x = np.concatenate((np.ones((self.train_x.shape[0], 1)), self.train_x), axis=1)\n",
    "        self.test_x = np.concatenate((np.ones((self.test_x.shape[0], 1)), self.test_x), axis=1)\n",
    "        f.close()\n",
    "        \n",
    "    @staticmethod\n",
    "    def shuffle(X, y):\n",
    "        \"\"\" Shuffle training data \"\"\"\n",
    "        shuffled_indices = np.random.permutation(len(y))\n",
    "        return X[shuffled_indices], y[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1 [5 points]**\n",
    "\n",
    "**Part 1 A [4 points]:** First, implement the `sigmoid` function to return the output by applying the sigmoid function $\\sigma(z)$ to the input parameter, where the sigmoid function $\\sigma(z)$ is defined as:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "513ed252f9bf919d7c5606e1eeaf9f7f",
     "grade": false,
     "grade_id": "cell-389deffcd0e8b595",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(score, threshold=20.0):\n",
    "    \"\"\"\n",
    "    Sigmoid function with a threshold\n",
    "    :param score: A real valued number to convert into a number between 0 and 1\n",
    "    :param threshold : Prevent overflow of exp by capping activation at 20.\n",
    "    return sigmoid function result.\n",
    "    \"\"\"\n",
    "    # TODO: Finish this function to return the output of applying the sigmoid\n",
    "    # function to the input score (Please do not use external libraries)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "363924b186c164d4105932c4dcaddfa0",
     "grade": true,
     "grade_id": "cell-73fb14b862081aa9",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# verify sigmoid implemention w/ scipy;\n",
    "# note: you should NOT use scipy for your implementation!\n",
    "from scipy.stats import logistic\n",
    "assert np.isclose(sigmoid(1), logistic.cdf(1))\n",
    "assert np.isclose(sigmoid(5), logistic.cdf(5))\n",
    "assert np.isclose(sigmoid(100, threshold=20), logistic.cdf(20))\n",
    "assert np.isclose(sigmoid(-1), logistic.cdf(-1))\n",
    "assert np.isclose(sigmoid(-5), logistic.cdf(-5))\n",
    "assert np.isclose(sigmoid(-100, threshold=20), logistic.cdf(-20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1 B [1 point]:**\n",
    "\n",
    "Next, implement the derivative of the `sigmoid` function, `sigmoid_grad`, i.e. $\\frac{\\partial\\sigma(x)}{\\partial x}$.\n",
    "\n",
    "Hint: your implementation of `sigmoid_grad` should be able to use  your `sigmoid` function to compute the derivative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f463d5a41efa533b8878eae4c4e45ba0",
     "grade": false,
     "grade_id": "cell-71a4ae76610d957e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_grad(y, threshold=20.0):\n",
    "    \"\"\"\n",
    "    Derivative/gradient of the sigmoid function.\n",
    "    :param y: A real valued input for which to compute the derivative.\n",
    "    :param threshold : Prevent overflow of exp by capping activation at 20.\n",
    "    return sigmoid derivative function result.\n",
    "    \"\"\"\n",
    "    # TODO: Finish this function to return the output of applying the sigmoid\n",
    "    # function to the input score (Please do not use external libraries)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f5dc02db6428084de5c0be23408bd3b",
     "grade": true,
     "grade_id": "cell-cac94bfa1729f2b9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# verify sigmoid_grad using numerical differentiation, i.e: f(x+h)-f(x-h) / 2h\n",
    "epsilon = 1.0E-8\n",
    "assert np.isclose(sigmoid_grad(1.0), (sigmoid(1.0 + epsilon) - sigmoid(1.0 - epsilon)) / (2.0*epsilon))\n",
    "assert np.isclose(sigmoid_grad(0.1), (sigmoid(0.1 + epsilon) - sigmoid(0.1 - epsilon)) / (2.0*epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 [35 points]**\n",
    "\n",
    "**Part 2 A [15 points]**\n",
    "\n",
    "The negative log likelihood objective is defined as:\n",
    "$$\n",
    "\\textrm{NLL}(\\boldsymbol{\\beta}) = -\\displaystyle\\sum_{i=1}^n \\left[y_i \\log \\sigma(\\boldsymbol{\\beta}^T{\\bf x}^{(i)}) + (1-y_i)\\log(1 - \\sigma(\\boldsymbol{\\beta}^T{\\bf x}^{(i)}))\\right] \n",
    "$$\n",
    "\n",
    "First, write down the derivative of the negative log likelihood objective function, with respect to $\\boldsymbol{\\beta}$. Since we are working with SGD, derive it for  $n=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13710555e242e51a990a84623f24e685",
     "grade": true,
     "grade_id": "cell-18bc846ce0e4d6b3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using the `sigmoid` function implemented earlier, finish the `sgd_update` function so that it performs stochastic gradient descent on the single training example and updates the weight vector correspondingly without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1467d149a25689f4769ee81c1230800b",
     "grade": false,
     "grade_id": "cell-26bd636741b8b10c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "class LogReg:\n",
    "    def __init__(self, num_features, eta):\n",
    "        \"\"\"\n",
    "        Create a logistic regression classifier\n",
    "        :param num_features: The number of features (including bias)\n",
    "        :param eta: A function that takes the iteration as an argument (the default is a constant value)\n",
    "        \"\"\"\n",
    "        self.w = np.zeros(num_features)\n",
    "        self.eta = eta\n",
    "\n",
    "    def progress(self, examples_x, examples_y):\n",
    "        \"\"\"\n",
    "        Given a set of examples, compute the probability and accuracy\n",
    "        :param examples: The dataset to score\n",
    "        :return: A tuple of (log probability, accuracy)\n",
    "        \"\"\"\n",
    "\n",
    "        logprob = 0.0\n",
    "        num_right = 0\n",
    "        for x_i, y in zip(examples_x, examples_y):\n",
    "            p = sigmoid(self.w.dot(x_i))\n",
    "            if y == 1:\n",
    "                logprob += math.log(p)\n",
    "            else:\n",
    "                logprob += math.log(1.0 - p)\n",
    "\n",
    "            # Get accuracy\n",
    "            if abs(y - p) <= 0.5:\n",
    "                num_right += 1\n",
    "\n",
    "        return logprob, float(num_right) / float(len(examples_y))\n",
    "\n",
    "    def sgd_update(self, x_i, y, lam = 0.0, current_iteration=None):\n",
    "        \"\"\"\n",
    "        Compute a stochastic gradient update to improve the log likelihood.\n",
    "        :param x_i: The features of the example to take the gradient with respect to\n",
    "        :param y: The target output of the example to take the gradient with respect to\n",
    "        :param lam : regularization term\n",
    "        :return: Return the new value of the regression coefficients\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Finish this function to do a single stochastic gradient descent update\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return self.w\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e65a1f5fe8453a2faa2ec2ae20d6542",
     "grade": true,
     "grade_id": "cell-028c202a2089443e",
     "locked": true,
     "points": 14,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from tests import tests\n",
    "tests.run_test_suite('prob 2A', LogReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 B [10 points]:** Complete the following code below to loop over the training data and perform stochastic gradient descent for the pre-defined number of epochs. You may try different combinations of `epochs` and learning rates `eta`.\n",
    "\n",
    "Note: remember to shuffle your training data using `Dataset.shuffle` at the beginning of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49f93f32f25b756a0cc065b2e9aae97b",
     "grade": true,
     "grade_id": "cell-a90025845b0cca0f",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO redefine learning rate and epochs accordingly\n",
    "eta  = 1e-3\n",
    "epochs = 500\n",
    "\n",
    "dataset_handler = Dataset('./data/titanic.pklz')\n",
    "lr = LogReg(dataset_handler.train_x.shape[1], eta)\n",
    "print('Train Data :', dataset_handler.train_x.shape)\n",
    "print('Test Data :', dataset_handler.test_x.shape)\n",
    "# Iterations\n",
    "iteration = 0\n",
    "accuracy_array = []\n",
    "for epoch in range(epochs):\n",
    "    # TODO: Finish the code to loop over the training data and perform a stochastic\n",
    "    # gradient descent update on each training example.\n",
    "    # NOTE: It may be helpful to call upon the 'progress' method in the LogReg class\n",
    "    # to make sure the algorithm is truly learning properly on both training and test data\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 C [5 points]:** Report train and test accuracy for the above experiments after 500 epochs with `eta` = 1e-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd767590869c90c0bde898740f270e2b",
     "grade": true,
     "grade_id": "cell-c906b4528206c03c",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 D [5 points]:** What is the role of the learning rate? What are the pros and cons of high/low learning rates? Do you see any trade-off? Plot accuracies of different $\\eta$s together vs. number of epochs for both training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b83d0d12d8db5e3cd7001abacfe77ae",
     "grade": true,
     "grade_id": "cell-09a16c6562efd6e5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dataset_handler = Dataset('./data/titanic.pklz')\n",
    "train_results = {}\n",
    "test_results = {}\n",
    "epochs = 1000\n",
    "for eta in [1e-3, 1e-4, 1e-5, 1e-6]:\n",
    "    lr = LogReg(dataset_handler.train_x.shape[1], eta)\n",
    "    test_accuracy_array =[]\n",
    "    train_accuracy_array = []\n",
    "    for epoch in range(epochs):\n",
    "        # TODO: Finish the code to loop over the training data and perform a stochastic\n",
    "        # gradient descent update on each training example.\n",
    "        # NOTE: It may be helpful to call upon the 'progress' method in the LogReg class\n",
    "        # to make sure the algorithm is truly learning properly on both training and test data\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "epochs_array = [i for i in range(0,1000,50)]\n",
    "epochs_array.append(1000)\n",
    "ax.plot(epochs_array, train_results[1e-3], color=\"steelblue\", label=str(1e-3))\n",
    "ax.plot(epochs_array, train_results[1e-4], color=\"lightblue\", label=str(1e-4))\n",
    "ax.plot(epochs_array, train_results[1e-5], color=\"grey\", label=str(1e-5))\n",
    "ax.plot(epochs_array, train_results[1e-6], color=\"black\", label=str(1e-6))\n",
    "ax.legend(loc=\"lower right\", fontsize=16)\n",
    "ax.set_xlabel(\"epochs\", fontsize=16)\n",
    "ax.set_ylabel(\"train accuracy\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot testing results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "epochs_array = [i for i in range(0,1000,50)]\n",
    "epochs_array.append(1000)\n",
    "ax.plot(epochs_array, test_results[1e-3], color=\"steelblue\", label=str(1e-3))\n",
    "ax.plot(epochs_array, test_results[1e-4], color=\"lightblue\", label=str(1e-4))\n",
    "ax.plot(epochs_array, test_results[1e-5], color=\"grey\", label=str(1e-5))\n",
    "ax.plot(epochs_array, test_results[1e-6], color=\"black\", label=str(1e-6))\n",
    "ax.legend(loc=\"lower right\", fontsize=16)\n",
    "ax.set_xlabel(\"epochs\", fontsize=16)\n",
    "ax.set_ylabel(\"test accuracy\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4b1d9f962a2ddedfdf79ec74c657b3c",
     "grade": true,
     "grade_id": "cell-dd0ea201d734cc98",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Part 2 E [10 points]**:\n",
    "\n",
    "Adding $l_2$ regularization to the feature parameters for NLL loss gives:\n",
    "\n",
    "$$\n",
    "\\textrm{NLL}_{l_2}(\\boldsymbol{\\beta}) = -\\displaystyle\\sum_{i=1}^n \\left[y_i \\log \\sigma(\\boldsymbol{\\beta}^T{\\bf x}^{(i)}) + (1-y_i)\\log(1 - \\sigma(\\boldsymbol{\\beta}^T{\\bf x}^{(i)}))\\right] + \\lambda\\displaystyle\\sum_{k=1}^{p} \\beta_{k}^2\n",
    "$$\n",
    "\n",
    "where $p$ is the number of features, and $\\beta_0$ is the bias term. Notice that $\\beta_0$ is not included in the regularization term.\n",
    "\n",
    "Write down the derivative of the regularized negative log likelihood loss function $\\textrm{NLL}_{l_2}$ with respect to $\\boldsymbol{\\beta}$. Since we are working with SGD, derive it for $n=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8de7f0f39e6af563561352399c29cb40",
     "grade": true,
     "grade_id": "cell-a25a04527ff025ff",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update your implementation of the `sgd_update` method so that it performs regularized SGD updates of the model parameters to minimize the regularized NLL loss function.\n",
    "\n",
    "Remember, do **not** regularize the bias parameter $\\beta_0$.\n",
    "\n",
    "Provide train and test accuracy after above change with `lam=1e-5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca0f630ec835ecbd8cbf7365b6cf9205",
     "grade": true,
     "grade_id": "cell-3879c5176d481064",
     "locked": true,
     "points": 9,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from tests import tests\n",
    "tests.run_test_suite('prob 2E', LogReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 F [5 points]** Plot accuracies of different $\\lambda$s together vs. epochs for both training and testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20ac26a9cd7dfc4ce9d426ad7d668dc3",
     "grade": true,
     "grade_id": "cell-a450215e16c0f1a5",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dataset_handler = Dataset('./data/titanic.pklz')\n",
    "train_results = {}\n",
    "test_results = {}\n",
    "epochs = 1000\n",
    "eta = 1e-5\n",
    "for lam in [0, 0.01, 0.1]:\n",
    "    lr = LogReg(dataset_handler.train_x.shape[1], eta)\n",
    "    test_accuracy_array =[]\n",
    "    train_accuracy_array = []\n",
    "    for epoch in range(epochs):\n",
    "        # TODO: Finish the code to loop over the training data and perform a stochastic\n",
    "        # gradient descent update with regularization on each training example.\n",
    "        # NOTE: It may be helpful to call upon the 'progress' method in the LogReg class\n",
    "        # to make sure the algorithm is truly learning properly on both training and test data\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "epochs_array = [i for i in range(0,1000,50)]\n",
    "epochs_array.append(1000)\n",
    "ax.plot(epochs_array, train_results[0], color=\"steelblue\", label=str(0))\n",
    "ax.plot(epochs_array, train_results[1e-1], color=\"black\", label=str(1e-1))\n",
    "ax.plot(epochs_array, train_results[1e-2], color=\"grey\", label=str(1e-2))\n",
    "ax.legend(loc=\"lower right\", fontsize=16)\n",
    "ax.set_xlabel(\"epochs\", fontsize=16)\n",
    "ax.set_ylabel(\"train accuracy\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot testing results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "epochs_array = [i for i in range(0,1000,50)]\n",
    "epochs_array.append(1000)\n",
    "ax.plot(epochs_array, test_results[0], color=\"steelblue\", label=str(0))\n",
    "ax.plot(epochs_array, test_results[1e-1], color=\"black\", label=str(1e-1))\n",
    "ax.plot(epochs_array, test_results[1e-2], color=\"grey\", label=str(1e-2))\n",
    "ax.legend(loc=\"lower right\", fontsize=16)\n",
    "ax.set_xlabel(\"epochs\", fontsize=16)\n",
    "ax.set_ylabel(\"test accuracy\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 F (continued)** What is the effect of regularization term with respect to accuracy? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8bdcea34f5386a716a6fd4f93bfa3ff9",
     "grade": true,
     "grade_id": "cell-0ce393d045320da5",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3 [5 points]** \n",
    "\n",
    "Time based Learning Rate is dynamic learning rate given the following equation:\n",
    "\n",
    "$\\textrm{LearningRate} = \\eta\\, / \\,(1 + \\textrm{decay} \\cdot \\textrm{current epoch})$\n",
    "\n",
    "Train SGD with the dynamic learning rate defined above and follow these instructions:\n",
    "* Use initial learning rate $\\eta = 0.1$.\n",
    "* Use $\\textrm{decay} = 0.001$.\n",
    "* Update learning rate `lr.eta` every epoch.\n",
    "* Plot train accuracy and learning rate together for each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4fa913dc8a117169071bc74b80601f0",
     "grade": true,
     "grade_id": "cell-4cc9c3502840b913",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "eta  = 1e-1\n",
    "epochs = 200\n",
    "dataset_handler = Dataset('./data/titanic.pklz')\n",
    "lr = LogReg(dataset_handler.train_x.shape[1], eta)\n",
    "learning_rates = []\n",
    "train_accuracy_array = []\n",
    "test_accuracy_array = []\n",
    "for epoch in range(epochs):\n",
    "    # TODO: Finish the code to loop over the training data and perform a stochastic\n",
    "    # gradient descent update on each training example with dynamic learning rate in each epoch.\n",
    "    # NOTE: It may be helpful to call upon the 'progress' method in the LogReg class\n",
    "    # to make sure the algorithm is truly learning properly on both training and test data\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "epochs_array = [i for i in range(0,200)]\n",
    "ax.plot(epochs_array, train_accuracy_array, color=\"steelblue\", label=str('train accuracy'))\n",
    "ax.plot(epochs_array, learning_rates,color=\"grey\", label=str('learning rate'))\n",
    "ax.legend(loc=\"upper right\", fontsize=16)\n",
    "ax.set_xlabel(\"epochs\", fontsize=16)\n",
    "ax.set_ylabel(\"\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Optional survey.\n",
    "***\n",
    "\n",
    "We are always interested in your feedback. At the end of each homework, there is a simple anonymous feedback [survey](https://forms.gle/sC8ysdoyaDbFbB959) to solicit your feedback for how to improve the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
